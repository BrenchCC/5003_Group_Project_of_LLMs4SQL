## 🧪 LLMs4SQL 项目视频解说稿

**（开场：项目标题页 + 小组成员信息）**  
**语速控制：** 约200字/分钟，总字数约1400字，时长约7分钟。

---

### 第一部分：研究背景与论文核心思想 (约1.5分钟)

**（画面：论文标题、摘要图示）**  
各位老师、同学，大家好。我们小组复现并扩展的项目是《Evaluating SQL Understanding in Large Language Models》。这篇由 Rahaman 等人发表于 EDBT 2025 的论文，核心目标是**系统性地评估大语言模型对 SQL 的“理解程度”**。

论文指出，尽管 LLM 在自然语言处理等领域表现出色，但其在结构化查询语言这类具有严格语法、语义和逻辑约束的任务上，是否具备真正的“理解能力”，仍是一个开放性问题。为此，作者提出了一个多维度的评估框架，通过五项关键任务来检验模型的四种核心能力：
- **识别能力**：如语法错误检测
- **上下文感知**：如缺失词元识别
- **语义理解**：如查询等价性判断、性能预测
- **逻辑连贯性**：如查询解释

论文的主要结论是：**现有模型在基础的识别和上下文任务上表现尚可，但在需要深度语义理解和逻辑连贯性的复杂任务上，所有模型均存在显著不足。** 这揭示了当前 LLM 在真正理解 SQL 语义方面的局限性。

---

### 第二部分：我们的实现思路与关键决策 (约2.5分钟)

**（画面：项目架构图、代码文件结构、Prompt设计示例）**  
在复现过程中，我们发现原论文缺乏一个**可工程化、可复现的评估管道**。因此，我们的核心工作并非简单重复实验，而是**构建一个标准化、可扩展的自动化评估框架**。我们的关键决策与创新点主要体现在三个方面：

**1. 工程化与标准化设计：**
- **统一接口封装**：我们开发了 `LLMServer` 类，统一封装了多家厂商（如字节、阿里、智谱、深度求索）的模型 API，支持“思考”与“非思考”两种推理模式，确保了实验的公平性与可复现性。
- **结构化输出约束**：我们通过精细的 **Prompt Engineering**，强制模型输出符合预定 JSON Schema 的结果。这解决了原论文中输出格式不统一、难以自动化解析的问题。
- **配置驱动实验**：所有实验参数，从模型配置到评估任务，均通过 YAML 文件管理，实现了灵活的“即插即用”式实验切换。

**2. 评估维度的细化与增强：**
- 我们对原论文的每项任务进行了更细粒度的拆解。例如，在“缺失词元识别”任务中，我们不仅评估模型能否发现错误（二分类），还评估其能否准确定位错误位置（回归问题）和判断错误类型（多分类）。
- 我们实现了自动化的评估管道，能够批量计算 **Precision、Recall、F1-Score、MAE（平均绝对误差）、Hit Rate** 等多种指标，并生成可视化图表。

**3. 模型选择与对比策略：**
- 我们选取了五款具有代表性的前沿模型进行横向对比，包括 Doubao、Qwen、GLM 以及开启/关闭“思考”模式的 DeepSeek 模型，以探究不同模型架构与推理能力对 SQL 理解的影响。

---

### 第三部分：实验结果与分析 (约2.5分钟)

**（画面：核心实验结果图表切换展示）**  
接下来，我将展示我们框架下的部分关键实验结果。

**首先，在语法错误检测任务上**，所有模型在判断“是否有错误”这个二分类任务上表现都较好（平均 F1-Score 在 0.8 左右）。然而，当任务升级为判断“具体是哪种语法错误”的多分类任务时，所有模型的性能都出现了**显著下降**（平均 F1-Score 降至 0.5-0.6）。这验证了**语义理解比语法识别更具挑战性**的论文观点。其中，GLM-4-6 模型在本任务中表现最为稳健。

**其次，在最具挑战性的查询性能预测任务上**，结果非常有趣。我们发现，**开启“思考”模式对于复杂任务至关重要**。以 DeepSeek 模型为例，开启思考模式后，其 F1-Score 从 0.40 大幅提升至 0.66，精确度提升了约 72%。这表明，链式推理能力能帮助模型更好地平衡精确度与召回率，做出更合理的判断。而非思考模型普遍表现出高召回率但低精确度的特点，说明它们更倾向于做出“保守”的预测。

**最后，在查询等价性判断任务上**，结论同样明确。所有模型在判断两个查询是否“等价”的二分类任务上尚可，但一旦需要区分“非等价”、“可能等价”、“结构等价”等细粒度类别时，性能便**急剧下滑**。这再次印证了论文的核心发现：**LLM 在处理深层次SQL语义和逻辑等价性时，能力依然薄弱。**

**（画面：总结性图表，如模型能力雷达图）**  
综合所有实验结果，我们可以得出一个清晰的结论：当前大语言模型的 SQL 理解能力存在明显的**任务分层效应**。它们在语法和浅层语义任务上已接近实用，但在需要深度推理和精准语义理解的复杂任务上，仍有巨大的提升空间。我们的评估框架清晰地量化了这一边界。

---

### 第四部分：总结与展望 (约0.5分钟)

**（画面：项目GitHub仓库链接、致谢）**  
总结来说，本项目不仅复现了一篇前沿论文，更重要的是，我们**构建了一个严谨、自动化、可扩展的LLM-SQL评估基准框架**。这项工作为未来更深入的研究（如模型微调、提示工程优化）提供了可靠的实验基础。

展望未来，基于LLM的智能数据库助手潜力巨大。我们相信，通过持续的性能优化、知识增强与应用探索，LLM有望从“SQL理解者”进化为真正的“数据库智能协作者”。

我们的项目代码已开源。以上就是我们小组的汇报，感谢各位的聆听，欢迎提问！

---